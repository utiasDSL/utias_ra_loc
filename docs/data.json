[{"layout":"default","title":"Dataset description","order":1,"graphics":true,"content":"<h1 id=\"range-aided-localization\">Range-aided localization</h1>\n\n<p>In range-based localization, a robot equipped with one or more distance sensors, referred to as a <em>tag</em>, determines it position by measuring the distance to other sensors in the environment, referred to as <em>anchors</em>, as shown below.</p>\n\n<p align=\"center\">\n <img src=\"assets/ra_loc.png\" alt=\"drawing\" width=\"400\" />\n</p>\n\n<p>If the position of the anchors is unknown, then the process of determining the robot trajectory <em>and</em> the anchor positions is generally referred to range-only (RO) simultaneous localization and mapping (SLAM).</p>\n\n<p>The distance measurements from a single tag are insufficient to determine the full pose (position and orientation) of the robot. Typically, range senors are combined with other sensing modalities such as inertial measurements (IMUs), wheel encoders and cameras to estimate the full pose, which is referred to as <em>range-aided</em> localization.</p>\n\n<h1 id=\"datasets\">Datasets</h1>\n\n<table>\n <thead>\n <tr>\n <th style=\"text-align: center\"><strong>Dataset</strong></th>\n <th style=\"text-align: center\"><strong>Location</strong></th>\n <th style=\"text-align: center\"><strong>Number of experiments</strong></th>\n <th style=\"text-align: center\"><strong>Link</strong></th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td style=\"text-align: center\"><a href=\"/utias_ra_loc/03_UTIAS_vicon_1212022.html#description\">UTIAS_vicon_12122022</a></td>\n <td style=\"text-align: center\">UTIAS Vicon tested</td>\n <td style=\"text-align: center\">4</td>\n <td style=\"text-align: center\"><a href=\"/utias_ra_loc/03_UTIAS_vicon_1212022.html#description\">link</a></td>\n </tr>\n <tr>\n <td style=\"text-align: center\"><a href=\"/utias_ra_loc/04_UTIAS_cafe_16122022.html#description\">UTIAS_cafe_16122022</a></td>\n <td style=\"text-align: center\">UTIAS Cafeteria</td>\n <td style=\"text-align: center\">4</td>\n <td style=\"text-align: center\"><a href=\"/utias_ra_loc/04_UTIAS_cafe_16122022.html#description\">link</a></td>\n </tr>\n <tr>\n <td style=\"text-align: center\"><a href=\"/utias_ra_loc/05_UTIAS_vicon_02022022.html\">UTIAS_vicon_02032022</a></td>\n <td style=\"text-align: center\">UTIAS Vicon tested</td>\n <td style=\"text-align: center\">16</td>\n <td style=\"text-align: center\"><a href=\"/utias_ra_loc/05_UTIAS_vicon_02022022.html#description\">link</a></td>\n </tr>\n </tbody>\n</table>\n\n<h2 id=\"dataset-description\">Dataset description</h2>\n\n<p>Each dataset consists of sensor data from three different sensing modalities:</p>\n\n<ol>\n <li>range or distance data from DW1000-based <a href=\"https://www.bitcraze.io/products/loco-positioning-node/\">Bitcraze Loco Positioning</a> ultrawideband (UWB) radios,</li>\n <li>body-referenced linear acceleration and angular velocities from an inertial measurement unit (IMU), and</li>\n <li>visual inertial odometry (VIO) from <a href=\"https://www.intelrealsense.com/tracking-camera-t265/\">Intel Realsense T265/T261 tracking camera</a>.</li>\n</ol>\n\n<p>The dataset was collected across multiple days on different robots in multiple indoor environments. Accordintly, the dataset is divided into multiple groups based on the environment in which it was collected.</p>\n\n<h3 id=\"ground-truth\">Ground truth</h3>\n<p>Each dataset has ground truth pose of the robot from a Vicon motion capture system for evaluation. In environments where a motion capture system is unavailable, ground truth position information from a Leica Total station is provided.</p>\n\n<h3 id=\"format\">Format</h3>\n\n<p>The sensor data from individual experiments is available in two formats: as <strong>rosbags</strong> and as <strong>csv</strong> files. When using rosbags, data from IMU, VIO and ground truth can be (de)serialized using standard ROS message definitions. Range data is provided via a custom message type: <code class=\"language-plaintext highlighter-rouge\">measurement_msgs/Range</code>. The ROS package for this custom message can be found here: <a href=\"https://github.com/utiasDSL/measurement_msgs\">link</a>. The message fields are defined as below:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>std_msgs/Header header\nstring mobile        # name of tag\nstring[] anchors     # name of anchor\nfloat32[] data       # measured distance\nfloat32[] covariance # measurement covariance\n</code></pre>  </div></div>\n\n<h3 id=\"calibration\">Calibration</h3>\n<p>A big part of achieving accurate localization involves calibrating the different sensors and their spatial and temporal offsets. We provide the spatial offsets of the different sensors with respect to the robot center in each case.</p>\n\n<p>The location of the anchors, body-referenced sensor spatial offsets, and sensor noise parameters are provided in the <code class=\"language-plaintext highlighter-rouge\">robot_config.yaml</code> file associated with each dataset.</p>\n\n<pre><code class=\"language-note\">The anchor configuration and sensor spatial offsets might be different for different datasets. Please use the config file associated with corresponding dataset.\n</code></pre>  \n","dir":"/","name":"02_description.md","path":"02_description.md","url":"/02_description.html"},{"layout":"default","title":"UTIAS_vicon_12122022","order":1,"graphics":true,"content":"<h1 id=\"utias_vicon_12122022\">UTIAS_vicon_12122022</h1>\n<p>The dataset was collected at the UTIAS vicon testbed equipped with 8 UWB anchors at the corners of a flight arena of dimensions 7m × 8m × 3.5m. The arena is equipped with a Vicon motion capture system for ground truth. A picture of the tested with the anchors and MAV highlighted is shown below.</p>\n\n<p align=\"center\">\n <img src=\"assets/utias_vicon_2023.png\" alt=\"drawing\" width=\"600\" />\n</p>\n\n<p>The test platform is a custom built 220mm size quadrotor with a single UWB tag, an Intel Realsense T261 tracking camera, and a Jetson TX1 computer. For each trial, the MAV was commanded along minimum-jerk trajectories. All the sensor data was recorded on the onboard computer in each case.</p>\n\n<p align=\"center\">\n <img src=\"assets/barbary.png\" alt=\"drawing\" width=\"400\" />\n</p>\n\n<h2 id=\"experiments\">Experiments</h2>\n\n<p>Videos of experiments involving four different paths are shown in the collage below.</p>\n\n<p align=\"center\">\n <img src=\"assets/utias_vicon.gif\" alt=\"drawing\" width=\"800\" />\n</p>\n\n<h2 id=\"data-files\">Data files</h2>\n\n<p>The dataset can be downloaded from here: <a href=\"\">UTIAS_vicon_12122022</a></p>\n","dir":"/","name":"03_UTIAS_vicon_1212022.md","path":"03_UTIAS_vicon_1212022.md","url":"/03_UTIAS_vicon_1212022.html"},{"layout":"default","title":"UTIAS_cafe_16122022","order":1,"graphics":true,"content":"<h1 id=\"utias_cafe_16122022\">UTIAS_cafe_16122022</h1>\n\n<h2 id=\"description\">Description</h2>\n\n<p>The dataset was collected at the UTIAS cafeteria.A panoramic view of the cafeteria is shown below. The dimensions of the cafeteria are 10m × 10m × 5m with 8 UWB anchors installed on the ceiling and on the floor. This is a challenging setup for UWB-based localization as all of the anchors are occluded by metallic and non-metallic objects that induce serve NLOS scenarios. The location of the anchors is highlighted by red circles.</p>\n\n<p align=\"center\">\n <img src=\"assets/utias_cafe.png\" alt=\"drawing\" width=\"800\" />\n</p>\n\n<p>The test platform is a custom built 220mm size quadrotor with a single UWB tag, an Intel Realsense T261 tracking camera, and a Jetson TX1 computer. For each trial, the MAV was commanded along minimum-jerk trajectories. Ground truth position is obtained by tracking the Lieca prism using a total station. The position offset of the Leica prism with respect to the body center is provided in the config file. All the sensor data was recorded on the onboard computer in each case.</p>\n\n<pre><code class=\"language-note\">In some trials, ground truth position may not be available for the entire trajectory due to the Total station losing track of the Leica prism.\n</code></pre>  \n\n<p align=\"center\">\n <img src=\"assets/barbary.png\" alt=\"drawing\" width=\"400\" />\n</p>\n\n<h2 id=\"experiments\">Experiments</h2>\n\n<p>Videos of experiments involving four different paths are shown in the collage below.</p>\n\n<p align=\"center\">\n <img src=\"assets/utias_cafe.gif\" alt=\"drawing\" width=\"800\" />\n</p>\n\n<h2 id=\"data-files\">Data files</h2>\n\n<p>The dataset can be downloaded from here: <a href=\"\">UTIAS_cafe_16122022</a></p>\n","dir":"/","name":"04_UTIAS_cafe_16122022.md","path":"04_UTIAS_cafe_16122022.md","url":"/04_UTIAS_cafe_16122022.html"},{"layout":"default","title":"UTIAS_vicon_02022022","order":1,"graphics":true,"content":"<h1 id=\"utias_vicon_02022022\">UTIAS_vicon_02022022</h1>\n\n<h2 id=\"description\">Description</h2>\n<p>The dataset was collected at the UTIAS vicon testbed equipped with 8 UWB anchors at the corners of a flight arena of dimensions 7m × 8m ×3.5 m. The arena is equipped with a Vicon motion capture system for ground truth. A picture of the tested with the anchors and MAV highlighed is shown below.</p>\n\n<p align=\"center\">\n <img src=\"assets/utias_vicon_2022.png\" alt=\"drawing\" width=\"600\" />\n</p>\n\n<p>The test platform is a custom built 220mm size quadrotor with a single UWB tag, an Intel Realsense T265 tracking camera, and an Intel UP board computer. For each trial, the MAV was flown manually along arbitrary trajectories. All the sensor data was recorded on the onboard computer in each case.</p>\n\n<p align=\"center\">\n <img src=\"assets/spedix250.png\" alt=\"drawing\" width=\"400\" />\n</p>\n\n<p>This dataset has been used in <a class=\"citation\" href=\"#gvi2022\">(Goudar et al., 2022)</a>, <a class=\"citation\" href=\"#safe_smooth\">(Dümbgen et al., 2023)</a> which serve as references for evaluation.</p>\n\n<h2 id=\"experiments\">Experiments</h2>\n<p>A video of one of the manual flights is shown below.</p>\n\n<p align=\"center\">\n <img src=\"assets/utias_vicon_2022.gif\" alt=\"drawing\" width=\"600\" />\n</p>\n\n<h2 id=\"data-files\">Data files</h2>\n\n<p>The dataset can be downloaded from here: <a href=\"\">UTIAS_vicon_02022022</a></p>\n\n<h2 id=\"references\">References</h2>\n<ol class=\"bibliography\"><li><span id=\"gvi2022\">Goudar, A., Zhao, W., Barfoot, T. D., &amp; Schoellig, A. P. (2022). Gaussian Variational Inference with Covariance Constraints Applied to Range-only Localization. <i>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 2872–2879. https://doi.org/10.1109/IROS47612.2022.9981520</span></li>\n<li><span id=\"safe_smooth\">Dümbgen, F., Holmes, C., &amp; Barfoot, T. D. (2023). Safe and Smooth: Certified Continuous-Time Range-Only Localization. <i>IEEE Robotics and Automation Letters</i>, <i>8</i>(2), 1117–1124. https://doi.org/10.1109/LRA.2022.3233232</span></li></ol>\n","dir":"/","name":"05_UTIAS_vicon_02022022.md","path":"05_UTIAS_vicon_02022022.md","url":"/05_UTIAS_vicon_02022022.html"},{"layout":"default","title":"Range-visual-inertial Localization","order":1,"graphics":true,"content":"<h1 id=\"range-visual-inertial-localization\">Range-visual-inertial localization</h1>\n\n<p>Coming soon…</p>\n","dir":"/","name":"06_rvi_loc.md","path":"06_rvi_loc.md","url":"/06_rvi_loc.html"},{"layout":"default","title":"Bibliography","order":1,"content":"<h2 id=\"references\">References</h2>\n\n<ol class=\"bibliography\"><li><span id=\"safe_smooth\">Dümbgen, F., Holmes, C., &amp; Barfoot, T. D. (2023). Safe and Smooth: Certified Continuous-Time Range-Only Localization. <i>IEEE Robotics and Automation Letters</i>, <i>8</i>(2), 1117–1124. https://doi.org/10.1109/LRA.2022.3233232</span></li>\n<li><span id=\"gvi2022\">Goudar, A., Zhao, W., Barfoot, T. D., &amp; Schoellig, A. P. (2022). Gaussian Variational Inference with Covariance Constraints Applied to Range-only Localization. <i>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 2872–2879. https://doi.org/10.1109/IROS47612.2022.9981520</span></li></ol>\n","dir":"/","name":"99_bibliography.md","path":"99_bibliography.md","url":"/99_bibliography.html"},{"layout":"default","title":"Range-Aided Localization","order":1,"content":"<h1 id=\"lsy-range-aided-localization-dataset\">LSY Range-Aided Localization Dataset</h1>\n\n<p align=\"center\">\n <img src=\"assets/ra_loc_collage.gif\" alt=\"drawing\" width=\"600\" />\n</p>\n\n<p>The Learning System and Robotics (LSY) lab range-aided localization dataset is a repostiory of datasets geared towards range-aided localization in indoor environments collected at the University of Toronto Institute for Aerospace Studies (UTIAS), Canada.</p>\n\n<p>A detailed description of the dataset can be found on page <a href=\"/utias_ra_loc/02_description.html#description\">Dataset description</a>.</p>\n\n<h2 id=\"citation\">Citation</h2>\n\n<p>If you use this dataset in your research, please use the following citation:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>@misc{agoudar2023raloc,\n      title = {LSY Range-Aided Localization Dataset}, \n      author = {Abhishek Goudar and Angela P. Schoellig},\n      year= {2023},\n      url = {https://utiasdsl.github.io/utias_ra_loc}\n}\n</code></pre>  </div></div>\n\n<h2 id=\"issues\">Issues</h2>\n<p>Please report any problems by reporting an issue on the <a href=\"https://github.com/utiasDSL/utias_ra_loc/issues\">GitHub Issue Tracker</a>.</p>\n\n<h2 id=\"license\">License</h2>\n\n<p>The dataset is released under <a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. Please contact us for commercial use.</p>\n","dir":"/","name":"index.md","path":"index.md","url":"/"}]